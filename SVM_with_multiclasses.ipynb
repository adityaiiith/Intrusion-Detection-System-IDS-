{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    },
    "colab": {
      "name": "SVM with multiclasses.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-A07llD_ytt"
      },
      "source": [
        "## Preliminaries\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets, preprocessing,cross_validation, feature_extraction\n",
        "from sklearn import linear_model, svm, metrics, ensemble, tree, ensemble\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import urllib\n",
        "import csv\n",
        "\n",
        "# Helper functions\n",
        "def folds_to_split(data,targets,train,test):\n",
        "    data_tr = pd.DataFrame(data).iloc[train]\n",
        "    data_te = pd.DataFrame(data).iloc[test]\n",
        "    labels_tr = pd.DataFrame(targets).iloc[train]\n",
        "    labels_te = pd.DataFrame(targets).iloc[test]\n",
        "    return [data_tr, data_te, labels_tr, labels_te]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4XPTnPu_ytu"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVgg9OqV_ytu"
      },
      "source": [
        "## Using SVM To Build The Model 5 class labeling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Kddi6x_ytu"
      },
      "source": [
        "#let's load the data\n",
        "train_data = urllib.urlopen('/home/sapt_rishi/Downloads/kddcup.data_10_percent_corrected')\n",
        "test_data = urllib.urlopen('/home/sapt_rishi/Downloads/corrected')\n",
        "\n",
        "#Place both dataset into a dataframe\n",
        "train_multiclass = pd.read_csv(train_data, quotechar=',', skipinitialspace=True, names=['Duration', 'protocol_type', 'Service', 'Flag', 'src_bytes', 'dst_bytes', 'Land', 'wrong_fragment', 'Urgent', 'Hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'Count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','Class'])\n",
        "test_multiclass = pd.read_csv(test_data, quotechar=',', skipinitialspace=True, names=['Duration', 'protocol_type', 'Service', 'Flag', 'src_bytes', 'dst_bytes', 'Land', 'wrong_fragment', 'Urgent', 'Hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'Count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','Class'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iypKXNNO_ytu"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXNv-q2I_ytu"
      },
      "source": [
        "## 1 Pre-Processing The Datasets:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RtcbU-X_ytu"
      },
      "source": [
        "### 1.1 Change Labels to The Right Class: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjQShuSw_ytu"
      },
      "source": [
        "## Replacing all the different attack types(24) to their proper general attack class\n",
        "train_multiclass.loc[(train_multiclass['Class'] =='smurf.')|(train_multiclass['Class'] =='neptune.') | (train_multiclass['Class'] =='back.') | (train_multiclass['Class'] =='teardrop.') |(train_multiclass['Class'] =='pod.')| (train_multiclass['Class']=='land.'),'Class'] = 'Dos'\n",
        "train_multiclass.loc[(train_multiclass['Class'] =='satan.')|(train_multiclass['Class'] =='ipsweep.') | (train_multiclass['Class'] =='portsweep.') | (train_multiclass['Class'] =='nmap.'),'Class'] = 'probe'\n",
        "train_multiclass.loc[(train_multiclass['Class'] =='spy.')|(train_multiclass['Class'] =='phf.')|(train_multiclass['Class'] =='multihop.')|(train_multiclass['Class'] =='ftp_write.') | (train_multiclass['Class'] =='imap.') | (train_multiclass['Class'] =='warezmaster.') |(train_multiclass['Class'] =='guess_passwd.')| (train_multiclass['Class']=='warezclient.'),'Class'] = 'r2l'\n",
        "train_multiclass.loc[(train_multiclass['Class'] =='buffer_overflow.')|(train_multiclass['Class'] =='rootkit.') | (train_multiclass['Class'] =='loadmodule.') | (train_multiclass['Class'] =='perl.'),'Class']='u2r'\n",
        "train_multiclass.loc[(train_multiclass['Class'] =='normal.'),'Class'] = 'normal'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqbC9Ok5_ytv"
      },
      "source": [
        "## Replacing all the different attack types(36) to their proper general attack class\n",
        "\n",
        "\n",
        "test_multiclass.loc[(test_multiclass['Class'] =='smurf.')|(test_multiclass['Class'] =='neptune.') | \n",
        "                    (test_multiclass['Class'] =='back.') | (test_multiclass['Class'] =='teardrop.') |\n",
        "                    (test_multiclass['Class'] =='pod.')| (test_multiclass['Class']=='land.')|\n",
        "                   (test_multiclass['Class']=='apache2.')|(test_multiclass['Class']=='udpstorm.')|\n",
        "                   (test_multiclass['Class']=='processtable.')|(test_multiclass['Class']=='mailbomb.'),'Class'] = 'Dos'\n",
        "\n",
        "\n",
        "test_multiclass.loc[(test_multiclass['Class'] =='guess_passwd.')|(test_multiclass['Class'] =='ftp_write.')|\n",
        "                    (test_multiclass['Class'] =='imap.')|(test_multiclass['Class'] =='phf.') | \n",
        "                    (test_multiclass['Class'] =='multihop.') | \n",
        "                    (test_multiclass['Class'] =='warezmaster.') |(test_multiclass['Class'] =='snmpgetattack.')| \n",
        "                    (test_multiclass['Class']=='named.')|(test_multiclass['Class'] =='xlock.')|\n",
        "                    (test_multiclass['Class'] =='xsnoop.')|(test_multiclass['Class'] =='sendmail.')|\n",
        "                    (test_multiclass['Class'] =='httptunnel.')|(test_multiclass['Class'] =='worm.')|\n",
        "                    (test_multiclass['Class'] =='snmpguess.'),'Class'] = 'r2l'\n",
        "\n",
        "test_multiclass.loc[(test_multiclass['Class'] =='satan.')|(test_multiclass['Class'] =='ipsweep.') | (test_multiclass['Class'] =='portsweep.') | (test_multiclass['Class'] =='nmap.')|\n",
        "                    (test_multiclass['Class'] =='saint.')|(test_multiclass['Class'] =='mscan.'),'Class'] = 'probe'\n",
        "\n",
        "test_multiclass.loc[(test_multiclass['Class'] =='buffer_overflow.')|(test_multiclass['Class'] =='rootkit.') | \n",
        "                    (test_multiclass['Class'] =='loadmodule.') | (test_multiclass['Class'] =='xterm.')|\n",
        "                    (test_multiclass['Class'] =='sqlattack.')|(test_multiclass['Class'] =='ps.')|\n",
        "                    (test_multiclass['Class'] =='perl.'),'Class']='u2r'\n",
        "\n",
        "test_multiclass.loc[(test_multiclass['Class'] =='normal.'),'Class'] = 'normal'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm5k7rby_ytv"
      },
      "source": [
        "### 2.2 Encoding The Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG6xRIOe_ytv",
        "outputId": "c4ecceb2-090d-44d1-f3cd-f1fc3fd34424"
      },
      "source": [
        "# Decoding The Dataset: \n",
        "attr_encoder = feature_extraction.DictVectorizer(sparse=False)\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "train_data_df_m = attr_encoder.fit_transform(train_multiclass.iloc[:,:-1].T.to_dict().values())\n",
        "train_target_df_m= label_encoder.fit_transform(train_multiclass.iloc[:,-1])\n",
        "\n",
        "\n",
        "train_data_decoded_m = pd.DataFrame(train_data_df_m)\n",
        "train_target_decoded_m = pd.DataFrame(train_target_df_m)\n",
        "\n",
        "test_data_df_m = attr_encoder.transform(test_multiclass.iloc[:,:-1].T.to_dict().values())\n",
        "test_target_df_m = label_encoder.transform(test_multiclass.iloc[:,-1])\n",
        "\n",
        "test_data_decoded_m = pd.DataFrame(test_data_df_m)\n",
        "test_target_decoded_m = pd.DataFrame(test_target_df_m)\n",
        "\n",
        "\n",
        "print train_data_decoded_m.shape\n",
        "print test_data_decoded_m.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(494021, 118)\n",
            "(311029, 118)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JysF7C7m_ytw"
      },
      "source": [
        "### 1.3 Perfroming Feature Reduction using PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f3vOfhN_ytw",
        "outputId": "e4c10271-e8c1-4021-93c9-9c3ec7182fcb"
      },
      "source": [
        "#load some modules to help\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "train_data_pca2 = PCA(n_components=29).fit_transform(train_data_decoded_m)\n",
        "test_data_pca2 = PCA(n_components=29).fit_transform(test_data_decoded_m)\n",
        "\n",
        "train_data_pca_df2 = pd.DataFrame(train_data_pca2)\n",
        "test_data_pca_df2 = pd.DataFrame(test_data_pca2)\n",
        "\n",
        "print train_data_pca_df2.shape\n",
        "print test_data_pca_df2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(494021, 29)\n",
            "(311029, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJK09BjI_ytw"
      },
      "source": [
        "### 1.4 Normalizing the Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCJfGgSj_ytw"
      },
      "source": [
        "#Creating our scaler and applyting it to our dataset after feature reduction\n",
        "standard_scaler = preprocessing.StandardScaler()\n",
        "train_ratio_standard_scaled_values2 = standard_scaler.fit_transform(train_data_pca_df2.values)\n",
        "train_data_scaled2=pd.DataFrame(train_ratio_standard_scaled_values2)\n",
        "\n",
        "test_ratio_standard_scaled_values2 = standard_scaler.fit_transform(test_data_pca_df2.values)\n",
        "test_data_scaled2=pd.DataFrame(test_ratio_standard_scaled_values2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGzMnap9_ytw"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bhioms1_ytw"
      },
      "source": [
        "## 2 Classification:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmO6RiZ2_ytw"
      },
      "source": [
        "### 2.1 Using SVM Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKLC4-F9_ytw",
        "outputId": "447cc221-04ea-432e-efb4-b9cf800d7b64"
      },
      "source": [
        "#Draft\n",
        "clf = svm.SVC(kernel='linear',class_weight=\"balanced\", max_iter=100000000)\n",
        "clf.fit(train_data_scaled2, train_target_decoded_m[0])\n",
        "clf_predict = clf.predict(test_data_scaled2)\n",
        "print clf.score(test_data_scaled2, test_target_decoded_m)\n",
        "print metrics.classification_report(test_target_decoded_m, clf_predict)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.88830623511\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.95      0.98      0.96    229853\n",
            "          1       0.80      0.85      0.82     60593\n",
            "          2       0.00      0.00      0.00      4166\n",
            "          3       0.01      0.00      0.00     16347\n",
            "          4       0.00      0.00      0.00        70\n",
            "\n",
            "avg / total       0.86      0.89      0.87    311029\n",
            "\n",
            "Number of support vectors for each class [ 831 5029   89  183   22]\n",
            "[[  3.06168543e-03  -2.62821770e-02   2.92121218e-02 ...,   5.25927608e+00\n",
            "   -1.17429443e+00  -6.21666293e+00]\n",
            " [  3.06168545e-03  -2.62823594e-02   3.05702387e-02 ...,   4.37957870e+00\n",
            "   -3.62719957e+00  -6.25108464e+00]\n",
            " [  2.01738062e-03  -2.62830927e-02   3.67396697e-02 ...,  -6.92651351e+00\n",
            "   -4.77964921e+00  -4.82940529e+00]\n",
            " ..., \n",
            " [  3.05763774e-03  -2.61610300e-02   2.85855914e-02 ...,   6.05223733e+00\n",
            "   -4.15001874e+00  -4.24851284e+00]\n",
            " [  1.53772805e-03   3.08304779e-02  -4.14853393e-01 ...,   1.90579151e+01\n",
            "   -1.03848798e+01  -4.81560377e+00]\n",
            " [  6.97832472e-04   1.00867055e-01  -3.37710746e-02 ...,   8.80253938e+00\n",
            "    8.42640634e-01  -3.81469791e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCLuIdgP_ytw"
      },
      "source": [
        "### 2.2 Using Decision Trees Algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI7moPoC_ytw"
      },
      "source": [
        "#### 2.2.1 Performaing Corss Validation on The Training Set for Testing Different Paramter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6Pi7ffq_ytw",
        "outputId": "c2478301-27ee-4ec7-ba09-4211774beaf6"
      },
      "source": [
        "## Testing SVM using Different Kernals with class weights balanced\n",
        "foldnum = 0\n",
        "fold_results = pd.DataFrame()\n",
        "criterion=[ 'gini','entropy']\n",
        "min_samples_leaf = [5,10]\n",
        "max_depth = [6,12]\n",
        "\n",
        "\n",
        "for cri in criterion:\n",
        "    for leaf in min_samples_leaf:\n",
        "        for depth in max_depth:\n",
        "            foldnum = 0\n",
        "            clf3 = tree.DecisionTreeClassifier(criterion=cri,min_samples_leaf=leaf,max_depth=depth,random_state=20160121,class_weight=\"balanced\")\n",
        "            for train, test in cross_validation.KFold(len(train_data_scaled2), n_folds=5,shuffle=True,random_state=20160202):  \n",
        "                [ids_tr_data, ids_te_data, ids_tr_target, ids_te_target] = folds_to_split(train_data_scaled_1,train_target_decoded,train, test)\n",
        "                clf3.fit(ids_tr_data, ids_tr_target[0])\n",
        "                fold_results.loc[foldnum, 'Accuracy'] = clf3.score(ids_te_data, ids_te_target)\n",
        "                foldnum+=1 \n",
        "            print \"criterion:\",cri\n",
        "            print \"min_samples_leaf:\",leaf\n",
        "            print \"max_depth:\",depth\n",
        "            print fold_results.mean()\n",
        "            print \"\\n\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "criterion: gini\n",
            "min_samples_leaf: 5\n",
            "max_depth: 6\n",
            "Accuracy    0.998128\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: gini\n",
            "min_samples_leaf: 5\n",
            "max_depth: 12\n",
            "Accuracy    0.999273\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: gini\n",
            "min_samples_leaf: 10\n",
            "max_depth: 6\n",
            "Accuracy    0.998146\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: gini\n",
            "min_samples_leaf: 10\n",
            "max_depth: 12\n",
            "Accuracy    0.999263\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 5\n",
            "max_depth: 6\n",
            "Accuracy    0.998889\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 5\n",
            "max_depth: 12\n",
            "Accuracy    0.99946\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 10\n",
            "max_depth: 6\n",
            "Accuracy    0.998911\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 10\n",
            "max_depth: 12\n",
            "Accuracy    0.999375\n",
            "dtype: float64\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FQNbfWp_ytx"
      },
      "source": [
        "#### 2.2.1 Testing the IDS Model on The Test Set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85NJwC59_ytx",
        "outputId": "dd7b5f96-d2e7-4d67-8083-da1535dbe8de"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "\n",
        "clf3 = tree.DecisionTreeClassifier(criterion = 'gini', min_samples_leaf = 5, max_depth=12,random_state=20160121,class_weight=\"balanced\")\n",
        "clf3.fit(train_data_scaled2, train_target_decoded_m[0])\n",
        "clf3_predict = clf3.predict(test_data_scaled2)\n",
        "print \"Accuracy :\", clf3.score(test_data_scaled2, test_target_decoded_m)\n",
        "print metrics.classification_report(test_target_decoded_m, clf3_predict)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.848322825203\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.89      0.96      0.92    229853\n",
            "          1       0.77      0.69      0.73     60593\n",
            "          2       0.11      0.15      0.13      4166\n",
            "          3       0.35      0.03      0.06     16347\n",
            "          4       0.00      0.00      0.00        70\n",
            "\n",
            "avg / total       0.82      0.85      0.83    311029\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weT4kTwn_ytx"
      },
      "source": [
        "### 2.3 Using Naive Bayes Algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXFxBGZB_ytx",
        "outputId": "dbf5d9e1-6033-4e62-e63d-32e551448801"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "y_pred = gnb.fit(train_data_scaled2, train_target_decoded_m[0])\n",
        "y_pred_predict3 = y_pred.predict(test_data_scaled2)\n",
        "print y_pred.score(test_data_scaled2, test_target_decoded_m)\n",
        "print metrics.classification_report(test_target_decoded_m, y_pred_predict3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.749843262204\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.96      0.78      0.86    229853\n",
            "          1       0.44      0.87      0.58     60593\n",
            "          2       0.00      0.00      0.00      4166\n",
            "          3       0.26      0.08      0.12     16347\n",
            "          4       0.75      0.09      0.15        70\n",
            "\n",
            "avg / total       0.81      0.75      0.76    311029\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YaWXzUT_ytx"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t87ykNY_ytx"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odU2w1km_ytx"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CD-Q0wg_ytx"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTaGnzz0_ytx"
      },
      "source": [
        "# Building 2 Class IDS Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J302fjS6_ytx"
      },
      "source": [
        "#let's load the data\n",
        "train_data_1 = urllib.urlopen('/home/sapt_rishi/Downloads/kddcup.data_10_percent_corrected')\n",
        "test_data_1 = urllib.urlopen('/home/sapt_rishi/Downloads/corrected')\n",
        "\n",
        "#Place both dataset into a dataframe\n",
        "train_class = pd.read_csv(train_data_1, quotechar=',', skipinitialspace=True, names=['Duration', 'protocol_type', 'Service', 'Flag', 'src_bytes', 'dst_bytes', 'Land', 'wrong_fragment', 'Urgent', 'Hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'Count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','Class'])\n",
        "test_class = pd.read_csv(test_data_1, quotechar=',', skipinitialspace=True, names=['Duration', 'protocol_type', 'Service', 'Flag', 'src_bytes', 'dst_bytes', 'Land', 'wrong_fragment', 'Urgent', 'Hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'Count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','Class'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VODxRDD5_ytx"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QghL2NUH_ytx"
      },
      "source": [
        "## 1 Pre-Processing The Datasets:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXmTuWN7_ytx"
      },
      "source": [
        "### 1.1 Converts Labels to The Right Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlVnymaa_ytx"
      },
      "source": [
        "train_class.loc[(train_class['Class'] !='normal.'),'Class'] = 'attack'\n",
        "train_class.loc[(train_class['Class'] =='normal.'),'Class'] = 'normal'\n",
        "\n",
        "test_class.loc[(test_class['Class'] !='normal.'),'Class'] = 'attack'\n",
        "test_class.loc[(test_class['Class'] =='normal.'),'Class'] = 'normal'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVW_gpRX_yty"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vquEZ_-b_yty"
      },
      "source": [
        "### 1.2 Encoding The Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXZp1XlW_yty",
        "outputId": "24c72ebb-7611-435a-cff1-d9fe8f2c7446"
      },
      "source": [
        "# Decoding The Dataset: \n",
        "attr_encoder = feature_extraction.DictVectorizer(sparse=False)\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "train_data_df = attr_encoder.fit_transform(train_class.iloc[:,:-1].T.to_dict().values())\n",
        "train_target_df= label_encoder.fit_transform(train_class.iloc[:,-1])\n",
        "\n",
        "\n",
        "train_data_decoded = pd.DataFrame(train_data_df)\n",
        "train_target_decoded = pd.DataFrame(train_target_df)\n",
        "\n",
        "test_data_df= attr_encoder.transform(test_class.iloc[:,:-1].T.to_dict().values())\n",
        "test_target_df= label_encoder.transform(test_class.iloc[:,-1])\n",
        "\n",
        "test_data_decoded = pd.DataFrame(test_data_df)\n",
        "test_target_decoded = pd.DataFrame(test_target_df)\n",
        "\n",
        "\n",
        "print train_data_decoded.shape\n",
        "print test_data_decoded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(494021, 118)\n",
            "(311029, 118)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af9B_s9__yty"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH7Co3HE_yty"
      },
      "source": [
        "### 1.3 Feature Reduction Using PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gg9lbDn_yty",
        "outputId": "70e7d00e-12ad-4733-cc45-5a3980a70947"
      },
      "source": [
        "#load some modules to help\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "train_data_pca_1 = PCA(n_components=29).fit_transform(train_data_decoded)\n",
        "test_data_pca_1 = PCA(n_components=29).fit_transform(test_data_decoded)\n",
        "\n",
        "train_data_pca_df_1 = pd.DataFrame(train_data_pca_1)\n",
        "test_data_pca_df_1 = pd.DataFrame(test_data_pca_1)\n",
        "\n",
        "print train_data_pca_df_1.shape\n",
        "print test_data_pca_df_1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(494021, 29)\n",
            "(311029, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv-6y-N1_yty"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt0St0v-_yty"
      },
      "source": [
        "### 1.4 Normalizing The Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGOfp7PE_yty"
      },
      "source": [
        "#Creating our scaler and applyting it to our dataset after feature reduction\n",
        "standard_scaler = preprocessing.StandardScaler()\n",
        "train_ratio_standard_scaled_values = standard_scaler.fit_transform(train_data_pca_df_1.values)\n",
        "train_data_scaled_1=pd.DataFrame(train_ratio_standard_scaled_values)\n",
        "\n",
        "test_ratio_standard_scaled_values = standard_scaler.fit_transform(test_data_pca_df_1.values)\n",
        "test_data_scaled_1=pd.DataFrame(test_ratio_standard_scaled_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERo2tB2L_yty"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT7RDgXr_yty"
      },
      "source": [
        "## 2 Classifiying The Data Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN8wyc3R_yty"
      },
      "source": [
        "### 2.1 Using SVM Algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z38ZwmgP_yty",
        "outputId": "8121d119-5de3-4556-908d-9e2e828fe450"
      },
      "source": [
        "#Draft\n",
        "lin = svm.SVC(kernel='linear', max_iter=100000000)\n",
        "lin.fit(train_data_scaled_1, train_target_decoded[0])\n",
        "lin_predict = lin.predict(test_data_scaled_1)\n",
        "print lin.score(test_data_scaled_1, test_target_decoded)\n",
        "print metrics.classification_report(test_target_decoded, lin_predict)\n",
        "print \"Number of support vectors for each class\", lin.n_support_\n",
        "print lin.support_vectors_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.922396303882\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.95      0.95      0.95    250436\n",
            "          1       0.80      0.80      0.80     60593\n",
            "\n",
            "avg / total       0.92      0.92      0.92    311029\n",
            "\n",
            "Number of support vectors for each class [1694 1688]\n",
            "[[  2.78947810e-03   4.43292344e-02  -6.15165127e-03 ...,   7.07344321e-01\n",
            "   -4.92057076e-01  -2.28549088e+00]\n",
            " [  2.01738092e-03  -2.62871757e-02   6.24009662e-02 ...,  -5.36280799e+00\n",
            "   -4.30704463e+00   2.56780715e+00]\n",
            " [  2.01738062e-03  -2.62830927e-02   3.67396697e-02 ...,  -6.92651351e+00\n",
            "   -4.77964921e+00  -4.82940529e+00]\n",
            " ..., \n",
            " [  1.54481243e-03   9.93833927e-02  -1.84981710e-02 ...,  -3.49703128e-01\n",
            "   -4.14043476e-02  -1.79917829e+00]\n",
            " [  2.82793121e-03  -1.56889839e-02   3.34062598e-02 ...,   7.36771477e-01\n",
            "    4.55919104e+00   9.46427430e-01]\n",
            " [  2.82388363e-03   3.23437103e-02   3.62694976e-02 ...,   4.35657953e-01\n",
            "    4.61212702e+00   9.38367066e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkhCD4_5_yty"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXyfoJ_T_yty"
      },
      "source": [
        "### 2.2 Using Decision Trees Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxOQe3lq_yty"
      },
      "source": [
        "#### 2.2.1 Performaing Corss Validation on The Training Set for Testing Different Paramter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2dwQcGr_yty",
        "outputId": "12752b55-b026-42d4-f93a-77b4b572cc49"
      },
      "source": [
        "## Testing SVM using Different Kernals with class weights balanced\n",
        "foldnum = 0\n",
        "fold_results = pd.DataFrame()\n",
        "criterion=[ 'gini','entropy']\n",
        "min_samples_leaf = [2, 5, 50]\n",
        "max_depth = [1,6,12]\n",
        "\n",
        "\n",
        "for cri in criterion:\n",
        "    for leaf in min_samples_leaf:\n",
        "        for depth in max_depth:\n",
        "            foldnum = 0\n",
        "            clf = tree.DecisionTreeClassifier(criterion=cri,min_samples_leaf=leaf,max_depth=depth,random_state=20160121,class_weight=\"balanced\")\n",
        "            for train, test in cross_validation.KFold(len(train_data_scaled_1), n_folds=5,shuffle=True,random_state=20160202):  \n",
        "                [ids_tr_data, ids_te_data, ids_tr_target, ids_te_target] = folds_to_split(train_data_scaled_1,train_target_decoded,train, test)\n",
        "                clf.fit(ids_tr_data, ids_tr_target[0])\n",
        "                clf_predict = clf.predict(ids_te_data)\n",
        "\n",
        "                fold_results.loc[foldnum, 'Accuracy'] = clf.score(ids_te_data, ids_te_target)\n",
        "                foldnum+=1 \n",
        "            print \"criterion:\",cri\n",
        "            print \"min_samples_leaf:\",leaf\n",
        "            print \"max_depth:\",depth\n",
        "            print fold_results.mean()\n",
        "            print \"\\n\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "criterion: gini\n",
            "min_samples_leaf: 2\n",
            "max_depth: 1\n",
            "Accuracy    0.969376\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: gini\n",
            "min_samples_leaf: 2\n",
            "max_depth: 6\n",
            "Accuracy    0.998154\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: gini\n",
            "min_samples_leaf: 2\n",
            "max_depth: 12\n",
            "Accuracy    0.999326\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: gini\n",
            "min_samples_leaf: 5\n",
            "max_depth: 1\n",
            "Accuracy    0.969376\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: gini\n",
            "min_samples_leaf: 5\n",
            "max_depth: 6\n",
            "Accuracy    0.998128\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: gini\n",
            "min_samples_leaf: 5\n",
            "max_depth: 12\n",
            "Accuracy    0.999273\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: gini\n",
            "min_samples_leaf: 50\n",
            "max_depth: 1\n",
            "Accuracy    0.969376\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: gini\n",
            "min_samples_leaf: 50\n",
            "max_depth: 6\n",
            "Accuracy    0.997601\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: gini\n",
            "min_samples_leaf: 50\n",
            "max_depth: 12\n",
            "Accuracy    0.998296\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 2\n",
            "max_depth: 1\n",
            "Accuracy    0.969376\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 2\n",
            "max_depth: 6\n",
            "Accuracy    0.998911\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 2\n",
            "max_depth: 12\n",
            "Accuracy    0.999547\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 5\n",
            "max_depth: 1\n",
            "Accuracy    0.969376\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 5\n",
            "max_depth: 6\n",
            "Accuracy    0.998889\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 5\n",
            "max_depth: 12\n",
            "Accuracy    0.99946\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 50\n",
            "max_depth: 1\n",
            "Accuracy    0.969376\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 50\n",
            "max_depth: 6\n",
            "Accuracy    0.998326\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "criterion: entropy\n",
            "min_samples_leaf: 50\n",
            "max_depth: 12\n",
            "Accuracy    0.998646\n",
            "dtype: float64\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlWtbJSG_ytz"
      },
      "source": [
        "#### 2.2.1 Testing the IDS Model on The Test Set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no_To1Us_ytz",
        "outputId": "81bb032b-619d-4a0c-8249-ed8f4e573874"
      },
      "source": [
        "\n",
        "clf_t = tree.DecisionTreeClassifier(criterion='entropy',min_samples_leaf=2,max_depth=12,random_state=20160121,class_weight=\"balanced\")\n",
        "clf_t.fit(train_data_scaled_1, train_target_decoded[0])\n",
        "clf_predict = clf_t.predict(test_data_scaled_1)\n",
        "\n",
        "print \"Accuracy (via score):\", clf_t.score(test_data_scaled_1, test_target_decoded)\n",
        "print metrics.classification_report(test_target_decoded, clf_predict)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy (via score): 0.824206102968\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.92      0.86      0.89    250436\n",
            "          1       0.54      0.67      0.60     60593\n",
            "\n",
            "avg / total       0.84      0.82      0.83    311029\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "554cduIh_ytz"
      },
      "source": [
        "### 2.3 Using Naive Bayes Algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqQ5cdcU_ytz",
        "outputId": "75602cd9-6b1f-4872-b0ef-3c82f430a4f2"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "Naive = gnb.fit(train_data_scaled_1, train_target_decoded[0])\n",
        "Naive_predict = Naive.predict(test_data_scaled_1)\n",
        "print Naive.score(test_data_scaled_1, test_target_decoded)\n",
        "print metrics.classification_report(test_target_decoded, Naive_predict)\n",
        "print Naive.class_prior_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.822302743474\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.87      0.91      0.89    250436\n",
            "          1       0.55      0.45      0.50     60593\n",
            "\n",
            "avg / total       0.81      0.82      0.81    311029\n",
            "\n",
            "[ 0.80308934  0.19691066]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}